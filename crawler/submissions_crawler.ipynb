{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd07839cf4c65842d51d8727e8d31cc7b71e238461db52811f472cb074731c49f23",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datetime\r\n",
    "import requests\r\n",
    "import json\r\n",
    "import re\r\n",
    "import time\r\n",
    "\r\n",
    "url = \"https://api.pushshift.io/reddit/search\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fetchObjects(**kwargs):\r\n",
    "    \r\n",
    "    # Default parameters\r\n",
    "    # Change as necessary/desired\r\n",
    "    params = {\r\n",
    "        \"sorted_type\": \"created_utc\",\r\n",
    "        \"sort\": \"asc\",\r\n",
    "        \"size\": \"1000\"\r\n",
    "        }\r\n",
    "    # define mode and pop from kwargs as it is not a valid url parameter    \r\n",
    "    mode = kwargs.pop('mode')\r\n",
    "\r\n",
    "    # Add additional parameters based on function arguments\r\n",
    "    for key, value in kwargs.items():\r\n",
    "        params[key] = value\r\n",
    "    \r\n",
    "    loop = True\r\n",
    "    while loop:\r\n",
    "        # Perform API request\r\n",
    "        r = requests.get(f'{url}/{mode}/', params=params, timeout=90)\r\n",
    "        # print(r.url)\r\n",
    "        if r.status_code != 200:\r\n",
    "            print(r.status_code)\r\n",
    "            print(\"Retrying...\")\r\n",
    "        else:\r\n",
    "            # successful (200), loop = False and process data\r\n",
    "            loop = False\r\n",
    "    else:\r\n",
    "        response = json.loads(r.text)\r\n",
    "        data = response['data']\r\n",
    "        sorted_data_by_id = sorted(data, key=lambda x: int(x['id'],36))\r\n",
    "        return sorted_data_by_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_reddit_data(**kwargs):\r\n",
    "\r\n",
    "    kwargs['after'] = int(datetime.datetime.strptime(kwargs['after'], '%d-%b-%Y').timestamp())\r\n",
    "    kwargs['before'] = int(datetime.datetime.strptime(kwargs['before'], '%d-%b-%Y').timestamp())\r\n",
    "\r\n",
    "    if 'after' in kwargs:\r\n",
    "        print(f\"Starting crawl from {datetime.datetime.fromtimestamp(kwargs['after'])}\")\r\n",
    "    else:\r\n",
    "        kwargs['after'] = 0\r\n",
    "    \r\n",
    "    max_id = 0\r\n",
    "\r\n",
    "    # Open file for JSON output\r\n",
    "    filepath = kwargs.pop('filepath')\r\n",
    "    file = open(filepath, \"a\")\r\n",
    "\r\n",
    "    while True: \r\n",
    "        nothing_processed = True\r\n",
    "        objects = fetchObjects(**kwargs)\r\n",
    "        if kwargs['after'] != '':\r\n",
    "            print(f\"Retrieving data from {datetime.datetime.fromtimestamp(kwargs['after'])}\")\r\n",
    "        \r\n",
    "        for object in objects:\r\n",
    "            id = int(object['id'],36)\r\n",
    "            if id > max_id:\r\n",
    "                nothing_processed = False\r\n",
    "                created_utc = object['created_utc']\r\n",
    "                max_id = id\r\n",
    "                if created_utc > kwargs['after']:\r\n",
    "                    kwargs['after'] = created_utc\r\n",
    "                # Output JSON data to the opened file\r\n",
    "                file.write(json.dumps(object,sort_keys=True,ensure_ascii=True) + \"\\n\")\r\n",
    "\r\n",
    "        # Exit if nothing happened\r\n",
    "        if nothing_processed: return\r\n",
    "        kwargs['after'] -= 1\r\n",
    "\r\n",
    "        # Sleep a little before the next function call\r\n",
    "        time.sleep(.5)\r\n",
    "    \r\n",
    "    file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extract_reddit_data(subreddit='coronavirus', mode='submission', after='20-JAN-2020', before='01-FEB-2021', filepath='submissions_dataset.json')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Function to resume crawl from the last row of data.\r\n",
    "Ensure that the file path is correct.\r\n",
    "'''\r\n",
    "\r\n",
    "# def resumeFromLast(**kwargs):\r\n",
    "#     filepath = kwargs['filepath']\r\n",
    "#     data = [json.loads(line) for line in open(filepath, 'r')]\r\n",
    "#     last_submission_date = data[-1]['created_utc']\r\n",
    "#     extract_reddit_data(after=last_submission_date, **kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Run method with the desired set of parameters\r\n",
    "'''\r\n",
    "# resumeFromLast(subreddit='coronavirus', mode='comment', before=1609516800, filepath='comments_dataset.json')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}